[全文介绍](https://zhuanlan.zhihu.com/p/111869532)

# On policy & Off policy

## Off-policy方法——将收集数据当做一个单独的任务

RL算法中需要带有随机性的策略对环境进行探索获取学习样本，一种视角是：off-policy的方法将收集数据作为RL算法中单独的一个任务，它准备两个策略：行为策略(behavior policy)与目标策略(target policy)。**行为策略是专门负责学习数据的获取，具有一定的随机性，**总是有一定的概率选出潜在的最优动作。而**目标策略借助行为策略收集到的样本以及策略提升方法提升自身性能，并最终成为最优策略。**Off-policy是一种灵活的方式，如果能找到一个“聪明的”行为策略，总是能为算法提供最合适的样本，那么算法的效率将会得到提升。

off-policy的话是：the learning is from the data **off** the **target policy**（引自《Reinforcement Learning An Introduction》）。也就是说RL算法中，数据来源于一个单独的用于探索的策略(不是最终要求的策略)。

### 重复性采样

假设已知随机策略π(a|s)，现在需要估计策略π对应的状态值V^π，但是只能用另一个策略 π'(a|s) 获取样本。对于这种需要用另外一个策略的数据(off-policy)来精确估计状态值的任务，需要用到重要性采样的方法，具体做法是在对应的样本估计量上乘上一个权重(π与π′的相对概率)，称为重要性采样率。

## On-policy——行为策略与目标策略相同

前面提到off-policy的特点是：the learning is from the data **off** the **target policy**，那么on-policy的特点就是：the target and the behavior polices are the same。也就是说on-policy里面只有一种策略，它既为目标策略又为行为策略。

# Online & Offline

online和offline的区别: 策略(价值函数)是不是step-by-step update的，如果是，就是online，不是就是offline.

## Online

根据Sutton经典教材的定义，online是step-by-step的同义词，agent与environment交互获得数据，立马更新策略(神经网络近似)或价值函数(终极目标是更新策略，价值函数也是用来更新策略的，它变相当于策略变)。TD算法就是online的，Q-learning 和 Sarsa 也是online的，因为他俩都基于TD算法。看看他们的伪代码就很清楚了。

## Offline

只要不是step-by-step更新的，就是offline了。蒙特卡洛方法就是offline的，是延迟更新的。它先跑完一个episode，一个episode包含多个steps。跑完一条，才用这条episode的数据更新经历过的状态的价值。它是先interaction完多步获得数据，再进行更新，而不是interaction一步，update一步。

# 激励函数

将线性输出，经过激励函数，使得原本的线性关系变成非线形

# 初始化

很多初始化策略都是为了保持每层的分布不变, 而BN是通过增加归一化层使得每层数据分布保持在N(0, 1)。xavier的初始化方式和BN一样，为了保证数据的分布（均值方差一致）。

```python
torch.nn.init.uniform_(tensor,a = 0, b = 1)
# 服从均匀分布~U(a,b)
torch.nn.init.normal_(tensor,	mean = 0, std = 1)
# 服从正态分布~U(mean, std)
torch.nn.init.constant_(tensor, val)
# 初始化为常数val

# xavier初始化 在tanh中表现出色，但是在Relu函数中表现很差
torch.nn.init.xavier_uniform_(tensor, gain)
# 均匀分布，计算公式 https://blog.csdn.net/weixin_54546190/article/details/122769883
torch.nn.init.xavier_normal_(tensor, gain)
# 正态分布

# kaiming
torch.nn.init.kaiming_uniform_(tensor, gain)
```

# TD-learning

[时序差分学习](https://zhuanlan.zhihu.com/p/33426502)

# torch.detach()

返回一个新的`tensor`，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个`tensor`永远不需要计算其梯度，不具有grad。

其实detach()和detach_()很像，两个的区别就是detach_()是对本身的更改，detach()则是生成了一个新的tensor。

## detach_()

其实就相当于变量之间的关系本来是x -> m -> y,这里的叶子tensor是x，但是这个时候对m进行了m.detach_()操作,其实就是进行了两个操作：

将m的grad_fn的值设置为None,这样m就不会再与前一个节点x关联，这里的关系就会变成x, m -> y,此时的m就变成了叶子结点
然后会将m的requires_grad设置为False，这样对y进行backward()时就不会求m的梯度

`grad_fn`为是用来记录计算过程的，反应梯度计算的过程



# 分层强化学习

目前的分层强化学习主要可以分为两大类，第一类是基于 **option** 的，第二类是基于 **goal** 的。实际上这两种方法并没有本质上的区别，这里的 **option** 表示的是一种具有 **时序抽象（temporal abstraction）**的策略，可以说是抽象出来的上层策略，这是策略层面上的定义；而 **goal** 则是目标层面上的定义，即智能体需要达到什么目标，每一层的不同目标同样对应着一个不同的子策略，这里的子策略其实就是option。依然拿做菜举例，为了做完一道菜，我们需要选择洗菜、切菜、炒菜，这里洗菜、切菜、炒菜均属于option，而把才洗干净、切细、炒熟则属于goal目标，可以看出两者是紧密联系的。总体来说，几乎所有的分层强化学习都是，上层控制器在较长的时间跨度里选择option/goal，下层控制器在较短的时间跨度里根据option/goal选择action。分层之所以能够提升样本效率，是因为上层控制器给下层控制器提供goal/option的同时还会根据下层控制器的策略好坏反馈一个对应的 **内在奖励（intrinsic reward）**，这就保证了即便在外部奖励为0的情况下，下层控制器依然能够获得奖励，从而一定程度上缓解了奖励稀疏的问题。

# 多智能体强化学习

[链接🔗](https://zhuanlan.zhihu.com/p/272735656)



# 链接地址

[1](https://zhuanlan.zhihu.com/p/478709774)

[Real-Time Scheduling for Dynamic Partial-No-Wait Multiobjective Flexible Job Shop by Deep Reinforcement Learning](https://zhuanlan.zhihu.com/p/457409248)

[强化学习在推荐系统](https://zhuanlan.zhihu.com/p/38875429)

AC[关键](https://zhuanlan.zhihu.com/p/110998399)

# DDPG

## 问题

[原文地址](https://blog.csdn.net/weixin_46133643/article/details/124356983)

Linear 层后，不实用BN层，使用LN层，否则整个训练过程很容易崩塌或者训练效果很差。

**实际上是看数据特征，当样本的特征差距过大，有些特征对于结果的影响就会非常小，减小特征之间的差距，选择BN。**

# TD3

[原文地址](https://blog.csdn.net/weixin_45492196/article/details/107866309)

# ε-greedy

```c++
def eps_greedy(width, heigth, S, action_all, eps, Q):
    # actions 构建动作空间space
    actions = [a for a in action_all if check_action(width, heigth, S, a)]
    # 计算行为的初始概率
    p = [eps/len(actions) for a in actions]
    # 选取Q值存储在q中
    q = [Q[S, a] for a in actions]
    # 找到具有最大Q值的动作所在的索引，将其对应的概率增加1 - eps，以增加其被选择的概率
    p[q.index(max(q))] += 1 - eps
    sum_p = 0
    random_value = random.random()
    for a, i in zip(actions, p):
        sum_p += i
        if sum_p >= random_value:
            # 累计概率大于等于随机值，则选择当前动作
            return a
```

**其中，不太明白为什么使用累计概率大于等于随机值的算法。**

在ε-greedy算法中，累计概率大于等于随机值的算法是用于选择动作的一种常见方法。它基于概率分布的思想，通过比较累计概率和随机值的大小来确定选择的动作。

首先，ε-greedy算法中的ε表示探索的程度，即以一定概率随机选择动作进行探索，而不仅仅依赖于当前估计的最优动作。具体而言，算法有两种选择策略：

1. 利用当前估计的最优动作选择：根据Q值函数的估计结果，选择具有最高Q值的动作作为最优动作。这样可以使算法倾向于选择当前估计的最优策略。

2. 随机选择动作进行探索：以一定的概率ε，随机选择一个动作，而不考虑其Q值大小。这样可以保证算法进行一定程度的探索，以发现可能更好的动作。

**在上述代码中，累计概率大于等于随机值的算法是用于在可行动作集合中选择动作。通过计算每个动作的累计概率，然后与随机值进行比较，可以按照概率选择动作。**

这种算法的优点是能够保证按照概率分布选择动作，以实现一定程度的探索和利用。较高的累计概率会增加相应动作的选择概率，从而增加了选择最优动作的可能性。较低的累计概率会降低相应动作的选择概率，从而保证了一定的随机性和探索性。

**总之，累计概率大于等于随机值的算法是ε-greedy算法中常用的一种策略选择方法，用于在利用和探索之间进行权衡，以实现动作选择的平衡和多样性。**

希望这个解释对您有所帮助！如果您还有其他问题，请随时提问。

# Github汇总

[多智能体强化学习代码](https://zhuanlan.zhihu.com/p/541151047)

[Single battery and single Day](https://github.com/wilsonhu7777/Vanilla-DQN-for-Bat)

[Microgrids_Project，一个相对来说可以实现的代码](https://github.com/Maxime00/Microgrids_Project/tree/main/Final_Model)

[MicroGrid_zhang614](https://github.com/zhang614/MicroGrid) 相当于SolarPrediction部分

[CS5890_MicroGrid](https://github.com/trentonamasa/CS5890_MicroGrid)：同样也是太阳能Solar和Battery两个部分

[Vanilla-DQN-for-Bat](https://github.com/wilsonhu7777/Vanilla-DQN-for-Bat)：DQN 单个电池🔋与市场电价直接的结合

[Enhancing-energy-trading](https://github.com/moayad-hsn/Enhancing-energy-trading-between-different-Islanded-Microgrids-A-Reinforcement-Learning-Algorithm) 跑不了，但是提供了pytorch版本ppo和DDPG算法

[Easygrid](https://github.com/YannBerthelot/easygrid) ：可以借鉴

[RL_for_MM](https://github.com/handongli2019/Reinforcement-learning-for-Microgrid-management)：风能，太阳能，停车车位占用情况

[pymgrid的风光电网](https://juejin.cn/post/7167635797317779487)

[Aidanessf电力模型](https://github.com/zidanessf/microgridOptimalDispatch)

[医院超市数据整理](https://github.com/kevinrussellmoy/AA222FinalProject)

[RL4MicroGrid](https://github.com/TanguyLevent/RL4Microgrids)

浙大代码[1](https://github.com/zidanessf/microgridOptimalDispatch)

浙大代码[2](https://github.com/xuhang1994/microgridtest_xuhang)

1zuu虚拟电厂包含该项目使用了 3 个电源 MicroGrid、SolarPower、[电池]()

[Dirkbig](https://github.com/dirkbig/GSy-sandbox-sim)
